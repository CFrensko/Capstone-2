{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project 2: Lending Club Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DESCRIPTIVE ABSTRACT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is your first time using the peer to peer platform Lending Club and you are looking to invest in a portfolio of loans with the highest rate of return. How do you know which loans to choose? What is the chance that the loan will go into default or get paid off? This report will explore the features of approved loans and create a model to predict the possibility of default.\n",
    "\n",
    "The data used is from 2015-2016 that consists of 421,097 rows with 145 columns. The data is taken from: https://www.lendingclub.com/info/download-data.action\n",
    "\n",
    "I will first apply data wrangling to clean up the data. Next I will do exploratory analysis by applying inferential statistics to better understand the data correlations and variables. After I will build a baseline model using logistic regression with hyper-parameter tuning using L1 and L2 regularization (regularization parameter). \n",
    "\n",
    "I will split the data into a training and test set to assess the performance of this model’s accuracy, precision, recall, specificity and F1 score. This will determine whether I should further explore other models such as random forest, KNN or decision tree. These results will all be tied together to see the impact each factor may have on overall price. The deliverables for this project will be the code and this report detailing the work completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lending Club is a peer to peer lending platform for loans up to $40,000. They were established in 2007 as a new way to provide credit with lower interest rates. A new customer can be approved in as little as 7 days with a payback period of 36 or 60 months. It offers investors access to the consumer credit asset class with the ability to make 3-8 percent returns. As lenders pay back their loan the ‘Lending Club Note’ investors receive monthly payments of both the principal and interest. If you invested in a bond you would not receive your principal back till maturity. \n",
    "\n",
    "The problem being solved is to identify whether the loan will be paid or go into default using a binary classification machine learning method. The client for this problem is Lending Club’s investors, the users lending money for the loans. This solution is important to identify for the client because the client needs a strong analysis and model for how the loans are predicted to perform. This will help the client determine which loan or group of loans to invest in based on their personal risk tolerance. We will need to identify how different loans do over time and determine a reasonable probability that a loan will default. We can use the approved loan features as support for Lending Club’s decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 APPROACH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach for solving this problem is to first apply data wrangling to clean up the data. Next do exploratory analysis by applying inferential statistics to better understand the data correlations and variables. Afterwards to build a baseline model using logistic regression with hyper-parameter tuning using L1 and L2 regularization (regularization parameter).\n",
    "\n",
    "Next the data will be split into a training and test set to assess the performance of this model’s accuracy, precision, recall, specificity and F1 score. This will determine whether it should further be explored using other models such as random forest, KNN or decision tree. These results will all be tied together to see the impact each factor may have on overall price. The deliverables for this project will be the code and this report detailing the work completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DATA SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 DATA ACQUISITION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used was pulled from lending club's website under lending club statistics for the year 2015-2016. The data was downloaded as a csv file and read as one large dataframe totaling 421,097 rows with 145 columns. Shown below is the distribution of loan amount by quantity and cost. The \n",
    "website the data was taken from: https://www.lendingclub.com/info/download-data.action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Distribution of Loan Amount Image](/Images/Distribution of Loan Amount.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2  DATA CLEANING & EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I began by performing exploratory data analysis to catch any discrepancies that would impact the model. I removed a multitude of columns that contained greater than 50% of missing data including: 'id','member_id','emp_title','url','desc','next_pymnt_d','mths_since_last_major_derog','mths_since_last_record','mths_since_recent_bc_dlq','mths_since_recent_revol_delinq','sec_app_open_acc','sec_app_revol_util','sec_app_open_act_il','sec_app_num_rev_accts','sec_app_chargeoff_within_12_mths','sec_app_collections_12_mths_ex_med','sec_app_mths_since_last_major_derog','hardship_type','hardship_reason','hardship_status','deferral_term','hardship_amount','hardship_start_date','hardship_end_date','payment_plan_start_date','hardship_length','hardship_dpd','mths_since_last_delinq','hardship_loan_status','orig_projected_additional_accrued_interest','hardship_payoff_balance_amount','hardship_last_payment_amount','debt_settlement_flag_date','settlement_status','settlement_date','settlement_amount','settlement_percentage','settlement_term','revol_bal_joint','sec_app_earliest_cr_line','sec_app_inq_last_6mths','open_acc_6m','open_act_il','open_il_12m','open_il_24m','mths_since_rcnt_il','il_util','open_rv_12m','open_rv_24m','max_bal_bc','total_bal_il','all_util','inq_fi','total_cu_tl','inq_last_12m','sec_app_mort_acc','annual_inc_joint','dti_joint', and 'verification_status_joint'.\n",
    "\n",
    "After reviewing the data available I came up with a subset of data useful for the analysis. I created a new dataframe labeled approved to further clean and perform logistic regression with. These categories included: 'grade','emp_length','home_ownership','verification_status','loan_status' and 'term'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Loan Status (Target Variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the loan status as the target variable for the logistic regression model. We can narrow it down into two categories of good loan and bad loan. A good loan would be a loan in good status. This would include those that are fully paid or current on their payments. A bad loan would be one that has gone bad with little possibility of being reversed. This would include a charged off loan (categorized as severely deliquent with little possibility of being paid off) or a defaulted loan. Those loans in the late or grace period categories will be removed since these loans could become a good or bad loan over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loan Distribution by Loan Status All Image](/Images/Loan Distribution by Loan Status All.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loan Status Image](/Images/Loan Status.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will label the 'Good Loans', those that are paid off or current with payments as '1'. The 'Bad Loans' or those that are in the charged off or in default status as '0'. The other categories of late or in grace period were removed. Below you can see that there are about 3.5 times more good loans (245493) than bad loans (69397)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loan Distribution by Loan Status All Image](/Images/Loan Distribution by Loan Status.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Verification Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The verification status indicates if income was verified by Lending Club, not verified, or if the income source was verified. I merged all those verified into one category labeled 1 and put all those unverified as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Distribution of Verification Status Image](/Images/Distribution of Verification Status.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Distribution of Verification Status Pie Chart Image](/Images/Distribution of Verification Status Pie Chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I created a stacked bar chart to look at the comparison of verified loans vs. good/bad loan status. There appears to be the same percentage of bad loans in both the verified and non-verified categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Verification by loan status Image](/Images/Verification by loan status.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Loan Grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are seven loan categories A to G. A is rated as the highest quality with G as the worst. First I looked at the loan grades vs loan amounts. There appeared to be many more loans approved for categories A-D than E-G."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loan grades vs loan amounts Image](/Images/Loan grades vs loan amounts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loan grades Image](/Images/Loan grades.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I next sorted the grades into numerical categories and compared them to the loan status. I found the loans categorized A to C have a low level of being a bad loan while D-G appear to have almost a 50% chance of being a bad loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loan status vs loan grade Image](/Images/Loan status vs loan grade.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Home Ownership"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The home ownership status was provided by the borrower during registration or obtained from the credit report. These values are: RENT, OWN, MORTGAGE, OTHER. I separated them into two categories of 1 signifying own/mortgage and 0 showing it is rent/any."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loan Distribution by Home Ownership Image](/Images/Loan Distribution by Home Ownership.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Home Ownership Image](/Images/Home Ownership.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then created a stacked bar graph to compare home ownership to the loan status. There appeared to be a larger number of bad loans in the rent/any category than the mortgage/own category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Home Ownership vs Loan Status Image](/Images/Home Ownership vs Loan Status.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.5 Employment Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employment length is listed in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years. In order to clean up the variables I changed 'n/a' to Nan. Since this category nan appears to be employment that isn't specified or that those unemployed did not list years of employment I replaced it with 0.\n",
    "\n",
    "To further clean I replaced the < (less than) sign with a space for less than one year. To simplify the categories I also combined less than a year with one year. Employment length was then sorted into 11 categories and I compared it to the target value of loan status. There did not seem to be any significant visible increase of default based on years of employment in the approved loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Employment Length Distribution.Image](/Images/Employment Length Distribution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Employment length vs loan status.Image](/Images/Employment length vs loan status.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.6 Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term is the number of payments on the loan. Values are in months and can be either 36 or 60. I separated and categorized the 36 month loans as 1 and the 60 month loans as 0. I then compared them to the loan status as good or bad. There was a significant larger number of defaults/Charge offs in the 60 month term vs the 36 month term. Considering many more loans were approved for 36 month term periods this may be an indicator of a good vs bad loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Term vs loan status.Image](/Images/Term vs loan status.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Terms Counts.Image](/Images/Terms Counts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.7 Dummy Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then created dummy variables for all of the categories, this lists all variables as 1 or 0 to prepare for the logistic regression model. The new data from consists of 314890 rows and 25 columns. The dataframes were then saved as csvs for easy access for future models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. INFERENTIAL STATISTICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end goal of performing a hypothsis test was to evaluate the variability of home ownership within the different loan status's in this data set. To see if there was significant variability between the two independent features in the data set. The two sample test test was applied to compare whether the average difference between the two groups was significant or if it was due instead to random chance. \n",
    "\n",
    "In this scenario since there was a large sample size with a binomial population ('1' and '0') but the standard deviation was unknown, we used a 2-sample t-test. With the large data set (>30) we can assume the means are normally distributed across the sample and the central limit theorem applies to this problem.\n",
    "\n",
    "In the dataset provided, each row represents a loan. The 'loan status separated' column has two values, '1' and '0', indicating good loan(paid/current) and bad loan (charged off/default). The column 'home ownership' has 2 values, 1 and 0, indicating whether the person taking the loan owns their home (1 being a home owner 0 being a renter or other). The end goal was to evaluate if loan status impacts the home ownership in this collected sample.\n",
    "\n",
    "The Null Hypothesis: Approved Good and Bad loans have the same home ownership values.\n",
    "The Alternative Hypothesis: Approved Good and Bad loans have different home ownership values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Compute margin of error, confidence interval and p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analysis on this data set it appears that home ownership had a significant value difference in good vs bad loans. The results from the tests show that the percentage of home ownership for good_loans was 61.17 percent and the percentage of home ownership for bad loans is 52.96 percent. This difference in percentage of home ownership is 8.2 percent.\n",
    "\n",
    "The t-statistic measures the size of the difference relative to the variation in the data. For this case the t-statistic was 38.5 with a p-value of zero. This means that the decision would be to reject the null hypothesis that the good and bad loans have the same ownership values. Also that the difference between the means of the test are statistically significant. In fact it is 38.5 times the ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Compute margin of error, confidence interval and p-value.Image](/Images/Compute margin of error, confidence interval and p-value.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Frequentist Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed a few tests to see if this difference could be perceived as random using a 2 sample t-test and frequentist bootstrapping. The p-value for the t-test and bootstrapping are both 0 so we can reject the null hypothesis. \n",
    "\n",
    "This would mean loan status is impacted by home ownership in this collected sample. There are however other variables (verification, grade, employment length, term) in the study that also should be taken into consideration in further investigation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Frequentist Bootstrapping.Image](/Images/Frequentist Bootstrapping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing our dataset is clean with the features free of excess noise we use supervised machine learning algorithms to build a predictive model. I first used logistic regression to classify and analyze the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The cleaned data was separated into a 80% training and 30% test set and the training data was then fit to the logistic regression model to predict loan status. The accuracy of logistic regression classifier on test set was 0.78446. At first glance this appears to be a good prediction but the results still needs further analysis.\n",
    "\n",
    "Next I tuned the model with 10 fold cross validation. Cross-validation is a technique used to evaluate the model by partitioning the original data set into training and test sets. In k-fold cross-validation such as this, the original sample was randomly partitioned into k equal size subsamples. The average accuracy was very close to the logistric regression classifier so we can conclude the model generalizes well.\n",
    "\n",
    "Next I used a grid of garameters to select the best feature c. The best regularization parameter for c was 1, althought 10 and 100 came in close. The purpose for this parameter is tuning unlikely high regression coefficients, preventing features from having terribly high weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Best C Values.Image](/Images/Best C Values.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the parameter of c = 1 was applied to the model it had no improvement on the score. This is because the default tuner for c is 1. Since this is only fit on one set of data we will next fit cross-validation and grid search on several sets of training data to further test for the optimal c parameter to use on the test set. The best parameter choosen when testing on multiple data sets was 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most performance metrics for the logistic regression model are based on the confusion matrix. It describes the number of true postives in the top left, true negatives in the bottom right, false positives in the top right and false negatives in the bottom left. True negatives and true positives are as they sound, that the values that were true or false were correctly identified/classified. False negatives and positives means they were misclassified.\n",
    "At first glance the false positives and negatives look high, we will use the next section to explain their significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Confusion Matrix.Image](/Images/Confusion Matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Precision, Recall, F-Measure & Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision tells us what percent described as defaulted or paid were actually defaulted or paid.\n",
    "For example this takes the true positive divided by the true positive and true negative. In this case the precision is 0.79 precise for paid but only 0.55 for defaulted. This means it is more precise for predicting paid loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall or sensitivity tells us what portion of those that defaulted or were paid were actually shown with the model as defaulted or paid. For example it takes the true positive divided by the true positive and false negative. The recall for those paid is 98 percent but for those that defaulted it is 9 percent. This shows an error in our model. The model does not predict well what we are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The f-measure tries to take on a harmonic mean between the precision and recall score to create one value to represent both and create a red flag if the model is not performing well. This gives us 88 percent for paid and 15 percent for defaulted. This supports our findings above that the model does not do well representing defaulted loans. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support is the number of occurrences of each class in the test set. As we can see the number of instances of defaulted loans was 20678 and the paid loans was 73789 in our test set. This shows we have a large difference in instances of each for classification with an almost 1:3 ratio.  This could suggest we have an imbalanced classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Precision, Recall, F-Measure & Support.Image](/Images/BPrecision, Recall, F-Measure & Support.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC value lies between 0.5 to 1 where 0.5 denotes a bad classifer and 1 denotes an excellent classifier. The area under the curve was 0.53 which is extremely close to 0.5. When AUC is approximately 0.5, the model has no discriminating capacity and is a poor model. This again shows we have an imbalanced classification problem and the model needs to be adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ROC Curve.Image](/Images/ROC Curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Hyperparameter Tuning with Regularization Parameter (L1 & L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 and L2 regularization are techniques used to reduce model overfitting. Overfitting is when a model is trained too much. It results in weights that fit the training data really well and when this model is applied to a new data set it will result in a poor prediction.\n",
    "\n",
    "When training a model the measure of error helps determine the right weights. Mean squared error is commonly used by finding the sum of squared differences between the output values for a set of weight values and the known, and divide by the sum of the number of training items. The idea of regularization is to penalize weight values by adding those weight values to the calculation of the error term.\n",
    "\n",
    "L1 weight regularization penalizes weight values by adding the sum of their absolute values to the error term. E = Sum(o - t)^2 / n + Sum(Abs(w))\n",
    "\n",
    "L2 weight regularization penalizes weight values by adding the sum of their squared values to the error term. E = Sum(o - t)^2 / n + Sum(w^2)\n",
    "\n",
    "When applying the penalty of L1 and L2 to the logistic regression there were no differences in the accuracy or F1 scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Improvement Summary: Imbalanced Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One of the problems with our data set is that the data is highly imbalanced with a 1:3 ratio. In this section I experimented with both under and over-sampling techniques paired with both logistic regression and random forest modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6.1 Random Over-Sampling with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fight the issue of imbalance one method is to generate new samples in the classes which are under-represented. One of the most naive strategies is to generate new samples by randomly sampling with replacement the current available samples. We started out with a training data set of [(0, 48719), (1, 171704)] and used random over-sampling to generate our new resampled data set [(0, 171704), (1, 171704)]. This resampled data will then be used to fit to the logistic regression classifier model and to generate a prediction of the test set.\n",
    "\n",
    "When comparing the results to not using over-sampling, the precision, recall and F1 scores are overall much better for the combination of defaulted vs. paid loans. The score for paid loans is significantly worse. This is okay because the model is now performing better because it is not generalizing or just assuming all loans are paid, leaving the small number of defaulted loans not accounted for. \n",
    "\n",
    "Precision which tells us what percent described as defaulted or paid were actually defaulted or paid is better for the paid loans but it is only catching 34 percent of the defaulted. Recall which tells us what portion of those that defaulted or were paid were actually shown with the model as defaulted or paid is much higher at 68 percent compared to the original 10 percent. The F1 score which is basically an overall average of these two signifiers was a small bit worse with a overall score of 72 percent to 67 percent (with defaulted improved and paid worse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Random Over-Sampling Logistic Regresion.Image](/Images/Random Over-Sampling Logistic Regresion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ROC Random Over-Sampling Logistic Regresion.Image](/Images/ROC Random Over-Sampling Logistic Regresion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6.2 Synthetic Minority Oversampling Technique (SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the RandomOverSampler is over-sampling by duplicating some of the original samples of the minority class, SMOTE generates new samples in by interpolation (adding data of a different nature than the original). Smote focuses on generating samples next to the original samples using a k-Nearest Neighbors classifier. I started out with a training data set of [(0, 48719), (1, 171704)] and used random over-sampling to generate our new resampled data set [(0, 171704), (1, 171704)]. This resampled data will then be used to fit to the logistic regression classifier model and to generate a prediction of the test set.\n",
    "\n",
    "We ended up getting the exate same results using SMOTE as Random Over Sampling. See 6.1 above for results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  6.3 Random Under-Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Under-Sampler is a fast and easy way to balance the data by randomly selecting a subset of data for the targeted classes. The original training data set was [(0, 48719), (1, 171704)] and after random under-sampling the resampled data set was [(0, 48719), (1, 48719)]. This resampled data will then be used to fit to the logistic regression classifier model and to generate a prediction of the test set.\n",
    "\n",
    "We ended up getting the exate same results as SMOTE & Random Over Sampling, except for a slightly higher overall F1 score of 74 percent compared to 73 percent. See 6.1 above for results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Random Over-Sampling with Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score for random over-sampling with random forest was about 65%. The precision, recall and F1 scores were very close to random over-sampling with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Over-sampling with Random Forest.Image](/Images/Over-sampling with Random Forest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Synthetic Minority Oversampling Technique (SMOTE) with Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score for SMOTE with random forest was about 65%. The precision, recall and F1 scores were very close to SMOTE with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SMOTE with Random Forest.Image](/Images/SMOTE with Random Forest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  6.6 Random Under-Sampling with Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score for random under-sampling with random forest was about 64%. The precision, recall and F1 scores were very close to random under-sampling with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Random Under-Sampling with Random Forest.Image](/Images/Random Under-Sampling with Random Forest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. CONCLUSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The problem being solved is to identify whether the loan will be paid or go into default using a binary classification machine learning method. Our results may be summarized as follows:\n",
    "* Determining the outcome for loans is difficult, especially when evaluating loans in an imbalanced data set (1:3 ratio) because they were approved loans that defaulted. By using the clean data set with the features grade, employment length, home ownership, verification status of income, loan status and term we came up with a reasonable model.\n",
    "* The best binary classification method for this imbalanced data set comparing the average F1 score was the Synthetic Minority Oversampling Technique (SMOTE) with the Random Forest model by the smallest fraction (68 percent).\n",
    "* The F1 score for this model's defaulted loans (0) was 45 percent and for the paid loans (1) was 74 precent. These results are better than just using logistic regression (0: 16 percent, 1: 88 percent). This means the model catches almost half of the approved loans that end up defaulting. \n",
    "* However there is still much room for improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 FUTURE WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since the focus of this work is to not only predict the defaulted loans for Lending Club's approved loans but also to understand how to improve the model for the sake of investing, more research is needed. In particular:\n",
    "* To further improve the model I would recommend using all the data of approved loans that lending club offers on its website from 2007-2018 to gather a larger more robust set of data.\n",
    "* It would also be helpful to evaluate other features in the set that would help improve the model such as debt to income ratio, employment title, or credit score (credit score was removed to be available as a public data set) in the next analysis.\n",
    "* Another variation that could be used would be to create a multi-variable classification model. Instead of just using the binary defaulted or paid categories we could also include current, in late payment or those in-grace period status.\n",
    "* This model can then be scaled to do future predictions on approved loans to evaluate the investment potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 CLIENT RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Once these improvement have been made, the model can be used to generate an even more accurate prediction of defaulted loans for Lending Club's approved loans.\n",
    "\n",
    "* Before you invest your money in the peer to peer Lending Club approved loans, check out the predicted default predictions for select grades of loans. Is it within your risk tolerance? If so what percent return can you expect from your loan?\n",
    "\n",
    "* It is possible Lending Club could incorporate these more in depth figures into the publically available assesments of their loans. You will anticipate trends in the data and have the confidence to invest more or stay skeptical.\n",
    "\n",
    "* Also if you are looking at taking out a loan Lending Club could incorporate this model for you to plug in your personal data to get an immediate estimated interest rate based on your predicted grade if approved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ACKNOWLEDGEMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special thanks to Springboard & my mentor AJ Sanchez! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
